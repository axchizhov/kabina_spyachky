# Кабина спячки

Это разбор статьи [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/abs/2208.12242) и тестирование их метода.

[Скрипт обучения модели](https://colab.research.google.com/github/axchizhov/kabina_spyachky/blob/main/train.ipynb) в коллабе.

[Логи экспериментов](https://wandb.ai/axchizhov/dreambooth-lora/table) в Wandb.

ИЛЛЮСТАРЦИЯ: сгенерированная фотка с матильдой

## Проблема

Навык рисования приобретается за 5-10 лет упорной работы. Это долго и сложно, поэтому большинство из нас рисовать не умеет.

В 1970х годах людям пришла в голову мысль — надо научить рисовать машину. Сначала результаты были весьма убогими, но прогресс не остановить. Десять лет назад появились первые серьезные успехи — искусственная нейросеть генерировала, хотя и с огрехами, правдоподобные фотографии и рисунки. А пару лет назад на свет появился подход, который позволяет создавать практически любые изображения по словесному описанию (text2image generation).

Трудной задачей пока остается генерация конкретных объектов (few-shot image generation). Машина легко нарисует человека в любой точке планеты и космоса, но вот просто нарисовать заданного человека или предмет ей пока сложно. 

Авторы Dreambooth решают эту задачу. Например, по небольшому набору изображений собаки (5-10 фото), они учатся генерировать эту собаку в новых ракурсах и положениях:

![task](assets/task.png)

## Метод решения

Задача решается файнтюнингом любой генеративной text2image модели. 

Метод основан на двух новых идеях:

1. Использование редкого токена в качестве метки субъекта
2. Функция потерь, которая предотвращает переобучение категории субъекта

Схема метода:
![схема](assets/approach.png)

Использование этих идей позволяет генерировать субъект в новых контекстах с хорошей точность. Кроме этого, существенно снизились требования к количеству экземпляров субъекта для обучения: со 100 до 3-5 штук.



## Эксперименты

Для обучения взял кадры с Матильдой из фильма Léon: The Professional.

Для тестов подготовил несколько наборов снимков. В качестве основного использовал этот:

![Mathilda](assets/Mathilda_orig.png)

В качестве обучающего текста взял "a photo of sks girl", а для валидации использовал "A photo of sks girl with a glock".

### Результаты

1. **Модель действительно адаптирует модель под субъект, даже без class-preservation loss**

Результаты генерации на разных эпохах обучения:

![validation_images](assets/validation_images.png)
![epochs_table](assets/epochs_table.png)

Видно, что у модели есть сложности с генерацией вариативных объектов. Нормальное оружие в руках с ходу не получилось сгенерировать — не помогла и конкретизация модели: "a gun" и "a glock" дают одинаково не похожие на оружие объекты. Впрочем, эта проблема присуща и всем остальным генеративным моделям.

Еще заметно, что без class-preservation loss после 300 эпох модель начинает заметно переобучаться — просто запоминает изображения и воспроизводит их с искажениями:

![600 epochs](assets/600_epochs.png)

2. **Совсем быстро обучить робастную модель не получится**

Рекомендованные 300 эпох на 4 изображениях пробегаются быстро, но для работы class-preservation loss требуется проходиться по еще 200-300 изображений, что сильно замедляет обучение. No free lunch :(

3. Замена stable-diffusion-1-5 на stable-diffusion-2-1


4. Снижение точности вычислений (fp16) вместе с оптимизированным механизмом внимания (xformers) дают почти двухкратный прирост в скорости обучения, и это только на малом количестве эпох. А вот подключение 8-битного Адама эффекта не дало.


Бюджет обучения я ограничил ресурсами Google Colab (~4-5 часов в сутки, по субъективным ощущениям).

Чтобы в него уложиться, быстренько опробовал вычислительные оптимизации, которые доступны из коробки.

![speed test](assets/speed.png)


5. Тюнинг текст энкодера

тут у меня закончился бюджет обучения

Что еще осталось не изученем:
- [ ] вариации prompt
- [ ] class preservation
- попробовать другой токен
- попробовать более простой аксессуар (вместо глока)
- опробовать разные наборы датасеетов

### Выводы


