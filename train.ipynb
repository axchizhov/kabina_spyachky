{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Download and install requirements\n",
    "\n",
    "# Install diffusers and fix the version\n",
    "!git clone https://github.com/huggingface/diffusers\n",
    "%cd diffusers\n",
    "!git checkout c4d282360184686f7d4a66787a4be9898cf1b8b0\n",
    "!pip install -e .\n",
    "\n",
    "# Install LoRa for Dreambooth requirements\n",
    "%cd examples/dreambooth/\n",
    "!pip install -r requirements.txt\n",
    "%cd ../../..\n",
    "\n",
    "# Get the data\n",
    "!git clone https://github.com/axchizhov/kabina_spyachky\n",
    "\n",
    "# Misc\n",
    "!pip install wandb bitsandbytes xformers\n",
    "\n",
    "# Setup\n",
    "import accelerate\n",
    "\n",
    "accelerate.utils.write_basic_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Set the hyperparameters\n",
    "\n",
    "# @markdown #### General settings\n",
    "model_name = \"stabilityai/stable-diffusion-2-1\" # @param {type:'string'}\n",
    "output_dir = \"output\" # @param {type:'string'}\n",
    "resolution = 768 # @param {type:'number'}\n",
    "epochs = 300 # @param {type:'number'}\n",
    "train_batch_size = 1 # @param {type:'number'}\n",
    "\n",
    "\n",
    "# @markdown #### Subject\n",
    "reference_subject_dir = \"/content/kabina_spyachky/Mathilda/small_same_clothes_cardigan_768\" # @param {type:'string'}\n",
    "subject_prompt = \"a photo of sks girl\" # @param {type:'string'}\n",
    "validation_prompt = \"A photo of sks girl with a glock\" # @param {type:'string'}\n",
    "\n",
    "\n",
    "# @markdown #### Prior preservation\n",
    "class_preservation_prompt=\"a photo of a girl\" # @param {type:'string'}\n",
    "generate_class_preservation_samples = 200 # @param {type:'number'}\n",
    "prior_loss_weight = 1.0 # @param {type:'number'}\n",
    "\n",
    "\n",
    "# @markdown #### Monitoring with W&B\n",
    "enable_monitoring = True # @param {type:\"boolean\"}\n",
    "# wandb_project_name = \"kabina_spyachky\" # @param {type:'string'}\n",
    "# wandb_notes = \"\" # @param {type:'string'}\n",
    "# wandb_tag = \"\" # @param {type:'string'}\n",
    "if enable_monitoring:\n",
    "    import wandb\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Dispaly reference suject images\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image_paths = Path(reference_subject_dir).glob(\"*.png\")\n",
    "\n",
    "resize_transform = transforms.Compose([\n",
    "    transforms.Resize((resolution, resolution)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "images = [resize_transform(Image.open(path)) for path in image_paths]\n",
    "grid = make_grid(images)\n",
    "img = transforms.ToPILImage()(grid)\n",
    "plt.imshow(img)\n",
    "print(reference_subject_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run the experiment\n",
    "\n",
    "script = \"diffusers/examples/dreambooth/train_dreambooth_lora.py\"\n",
    "steps = int(epochs * len(images) / train_batch_size)\n",
    "\n",
    "!accelerate launch --mixed_precision=fp16 $script \\\n",
    "  --pretrained_model_name_or_path=$model_name  \\\n",
    "  --instance_data_dir=$reference_subject_dir \\\n",
    "  --output_dir=$output_dir \\\n",
    "  --instance_prompt=\"$subject_prompt\" \\\n",
    "  --resolution=$resolution \\\n",
    "  --train_batch_size=$train_batch_size \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --checkpointing_steps=100 \\\n",
    "  --learning_rate=1e-4 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=$steps \\\n",
    "  --validation_prompt=\"$validation_prompt\" \\\n",
    "  --validation_epochs=10 \\\n",
    "  --seed=\"42\" \\\n",
    "  --report_to=\"wandb\" \\\n",
    "  --enable_xformers_memory_efficient_attention\n",
    "\n",
    "\n",
    "  # --class_prompt=\"$class_preservation_prompt\" \\\n",
    "  # --with_prior_preservation \\\n",
    "  # --prior_loss_weight=$prior_loss_weight \\\n",
    "  # --num_class_images=$generate_class_preservation_samples \\\n",
    "  # --sample_batch_size=4 \\\n",
    "  # --class_data_dir=\"class_preservation_samples\"\n",
    "  # --use_8bit_adam\n",
    "  # --gradient_accumulation_steps=2 --gradient_checkpointing \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Inference. Play with the model\n",
    "\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "inference_prompt = 'A photo of a sks girl in a bucket' #@param {type:\"string\"}\n",
    "guidance_scale = 1.4 #@param {type:\"slider\", min:0, max:15, step:0.2}\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(model_name, torch_dtype=torch.float16, use_safetensors=True)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "pipe.unet.load_attn_procs(output_dir)\n",
    "\n",
    "image = pipe(inference_prompt, num_inference_steps=25, guidance_scale=1.5).images[0]\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
